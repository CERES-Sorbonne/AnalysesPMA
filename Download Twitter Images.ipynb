{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e0c5e1",
   "metadata": {},
   "source": [
    "## Download Images from tweet ids\n",
    "\n",
    "This notebook aims to download images using the api, based on the tweets ids that were collected with DMI TCAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c801057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "import time\n",
    "import hashlib\n",
    "import concurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c066e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_search = {\n",
    "    \"tweet.fields\": \"public_metrics,referenced_tweets,possibly_sensitive,created_at,source,reply_settings,withheld\",\n",
    "    \"expansions\": \"author_id,in_reply_to_user_id,attachments.media_keys\",\n",
    "    \"media.fields\": \"url,public_metrics,type,alt_text\",\n",
    "    \"user.fields\": \"id,verified,name\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb25fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_FOLDER = \"/home/tyra/Documents/CERES/resources\"\n",
    "OUTPUT_FOLDER = \"/home/tyra/Documents/CERES/PMA/MPT\"\n",
    "CREDENTIALS_FILES = r\"/home/tyra/Documents/CERES/credentials_pro.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7a0aec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token():\n",
    "    with open(CREDENTIALS_FILES, 'r') as f:\n",
    "        return f\"Bearer {json.load(f)['token']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f6215bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = requests.Session()\n",
    "s.headers.update({\"Authorization\": generate_token()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bcdd5ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131071"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open the json containing all the ids that we want to fetch\n",
    "with open(os.path.join(ROOT_FOLDER, 'pma.json'), 'r') as f:\n",
    "    ids = json.load(f)\n",
    "    ids = [str(i) for i in ids]\n",
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f305f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_media(media_key=None, url=None, **kwargs):\n",
    "    if not media_key or not url:\n",
    "        raise ValueError(\"Missing field when trying to save media\")\n",
    "    file_type = url.split('.')[-1]\n",
    "    sha1 = None\n",
    "    # download the file\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "    except requests.RequestException:\n",
    "        raise ValueError(f\"There was an error when downloading the media with following url: {url}, please check your connection or url\")\n",
    "    \n",
    "    # calculate signature of content, if this signature already exists then just increment the number of \n",
    "    buffer = res.content\n",
    "    signature = hashlib.sha1(buffer).hexdigest()\n",
    "    file_name = f\"{signature}.{file_type}\"\n",
    "\n",
    "    with open(os.path.join(OUTPUT_FOLDER, 'media', file_name), 'wb') as f:\n",
    "        f.write(res.content)\n",
    "        \n",
    "    with open(os.path.join(OUTPUT_FOLDER, 'sha1.json'), 'r') as f:\n",
    "        sha1 = json.load(f)\n",
    "    \n",
    "    sha1[media_key] = file_name\n",
    "    \n",
    "    with open(os.path.join(OUTPUT_FOLDER, 'sha1.json'), 'w') as f:\n",
    "        json.dump(sha1, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaf7b16",
   "metadata": {},
   "source": [
    "Save all the tweets that were already fetched so in case of error, we can just continue from where we stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "152639c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70200"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    with open(os.path.join(OUTPUT_FOLDER, 'fetched.json'), 'r') as f:\n",
    "        fetched = json.load(f)\n",
    "        already_parsed = len(fetched)\n",
    "        nb_calls = round((len(ids) - already_parsed) / 100)\n",
    "except:\n",
    "    fetched = []\n",
    "    already_parsed = 0\n",
    "    nb_calls = round((len(ids) - already_parsed) / 100)\n",
    "\n",
    "already_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2576eed9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|█████████████████████████████████▏       | 493/609 [14:55<02:50,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Too Many Requests', 'detail': 'Too Many Requests', 'type': 'about:blank', 'status': 429}\n",
      "making a break\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 609/609 [34:06<00:00,  3.36s/it]\n"
     ]
    }
   ],
   "source": [
    "for occ in tqdm.tqdm(range(nb_calls)):\n",
    "        \n",
    "    # fetch ids 100 by 100\n",
    "    ids_to_fetch = ids[already_parsed + occ * 100: already_parsed + occ * 100 + 100]    \n",
    "    res = s.get('https://api.twitter.com/2/tweets?ids=' + ','.join(ids_to_fetch), params=params_search)\n",
    "    \n",
    "    # we made too many calls, lets wait 15min\n",
    "    if 'data' not in res.json():\n",
    "        print(res.json())\n",
    "        print('making a break')\n",
    "        time.sleep(900)\n",
    "        res = s.get('https://api.twitter.com/2/tweets?ids=' + ','.join(ids_to_fetch), params=params_search)\n",
    "    \n",
    "    # get results\n",
    "    tweets = res.json()['data']\n",
    "    media = res.json()['includes'].get('media', None)\n",
    "    for tweet in tweets:\n",
    "        # save all medium info in the tweet json\n",
    "        for index, key in enumerate(tweet.get('attachments', {}).get('media_keys', [])):\n",
    "            for medium in media:\n",
    "                if medium['media_key'] == key:\n",
    "                    tweet['attachments']['media_keys'][index] = medium\n",
    "                    break\n",
    "        try:\n",
    "            with open(os.path.join(OUTPUT_FOLDER, f\"{tweet['id']}.json\"), 'w') as f:\n",
    "                json.dump(tweet, f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "#         # get the origin tweet if tweet is a retweet --> update that's too much requests\n",
    "#         for rtweet in tweet.get('referenced_tweets', []):\n",
    "#             if rtweet['type'] == 'retweeted' and rtweet['id'] not in fetched:\n",
    "#                 res = s.get(f\"https://api.twitter.com/2/tweets/{rtweet['id']}\", params=params_search)\n",
    "#                 if 'data' not in res.json():\n",
    "#                     print(res.json())\n",
    "#                 with open(os.path.join(OUTPUT_FOLDER, f\"{rtweet['id']}.json\"), 'w') as f:\n",
    "#                     json.dump(res.json()['data'], f)\n",
    "#                 fetched.append(rtweet['id'])\n",
    "#     if media:\n",
    "#         for medium in media:\n",
    "#             # check if all media were properly downloaded first time\n",
    "#             in_sha_1 = medium['media_key'] in sha1\n",
    "#             if not in_sha_1:\n",
    "#                 missing_media.append(medium)\n",
    "#                 with open(os.path.join(OUTPUT_FOLDER, 'missing_media.json'), 'w') as f:\n",
    "#                     json.dump(missing_media, f)\n",
    "    if media:\n",
    "        for medium in media:\n",
    "            if medium['type'] == 'photo':\n",
    "                download_media(medium['media_key'], medium['url'])\n",
    "                \n",
    "    # write fetch only if everything was written\n",
    "    fetched = [*fetched, *ids_to_fetch]\n",
    "    with open(os.path.join(OUTPUT_FOLDER, 'fetched.json'), 'w') as f:\n",
    "        json.dump(fetched, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "83f2642f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1652 443 90\n"
     ]
    }
   ],
   "source": [
    "missing_videos = [m for m in missing_media if m['type'] == 'video']\n",
    "missing_gif = [m for m in missing_media if m['type'] == 'animated_gif']\n",
    "missing_photos = [m for m in missing_media if m['type'] == 'photo']\n",
    "print(len(missing_videos), len(missing_gif), len(missing_photos))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
